{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of stopwords\n",
    "stopwords = set([\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"aren\",\"'t\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can\",\"cannot\",\"could\",\"couldn\",\"did\",\"didn\",\"do\",\"does\",\"doesn\",\"doing\",\"don\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"hadn\",\"has\",\"hasn\",\"have\",\"haven\",\"having\",\"he\",\"he\",\"'d\",\"he\",\"'ll\",\"he\",\"'s\",\"her\",\"here\",\"here\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"how\",\"i\",\"'m\",\"'ve\",\"if\",\"in\",\"into\",\"is\",\"isn\",\"it\",\"its\",\"itself\",\"let\",\"'s\",\"me\",\"more\",\"most\",\"mustn\",\"my\",\"myself\",\"no\",\"nor\",\"not\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"same\",\"shan\",\"she\",\"she\",\"'d\",\"she\",\"ll\",\"she\",\"should\",\"shouldn\",\"so\",\"some\",\"such\",\"than\",\"that\",\"that\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"there\",\"these\",\"they\",\"they\",\"they\",\"they\",\"'re\",\"they\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"very\",\"was\",\"wasn\",\"we\",\"we\",\"we\",\"we\",\"we\",\"'ve\",\"were\",\"weren\",\"what\",\"what\",\"when\",\"when\",\"where\",\"where\",\"which\",\"while\",\"who\",\"who\",\"whom\",\"why\",\"why\",\"with\",\"won\",\"would\",\"wouldn\",\"you\",\"your\",\"yours\",\"yourself\",\"yourselves\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in  spam and ham data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(trainingdir, stemming=False, lowercase=False, silently=False):\n",
    "    print(f\"Stemming is {stemming}\")\n",
    "    print(f\"Lowercase is {lowercase}\")\n",
    "    train_set, train_labels = reader.load_dataset_main(trainingdir,stemming,lowercase,silently)\n",
    "    return train_set, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dir = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming is False\n",
      "Lowercase is False\n"
     ]
    }
   ],
   "source": [
    "train_set, train_labels = load_data(training_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5172"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)\n",
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Subject christmas tree farm pictures'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = []\n",
    "for lst in train_set:\n",
    "    corpus.append(' '.join(lst))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ThinkPad\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['re', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5172, 50322)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopwords)\n",
    "tf_idf_score = vectorizer.fit_transform(corpus)\n",
    "print(tf_idf_score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5172, 50322)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "normalized_matrix = normalize(tf_idf_score, norm='l1', axis=1)\n",
    "\n",
    "print(normalized_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50322)\n"
     ]
    }
   ],
   "source": [
    "# Stores all th enormalized tf-idf scores\n",
    "normalized_tf_idf = np.sum(normalized_matrix, axis=0)\n",
    "\n",
    "print(normalized_tf_idf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50322,)\n"
     ]
    }
   ],
   "source": [
    "words = vectorizer.get_feature_names_out()\n",
    "print(words.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
